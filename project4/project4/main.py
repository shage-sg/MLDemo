# -*- coding: utf-8 -*-import numpy as npimport torch as thimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as pltfrom mnist_loader import load_mnistfrom dataset import MNISTDatasetnp.random.seed(1)th.manual_seed(1)sns.set_style("darkgrid")#%% 载入数据train_image, train_label = load_mnist(dataset="training", path="mnist")test_image, test_label = load_mnist(dataset="testing", path="mnist")# train_image.shape, train_label.shape# test_image.shape, test_label.shape#%% 数据可视化from utils import show_mnist# 以 5x5 矩阵展示手写数字图像show_mnist(train_image, train_label, 5, 5)# 降维后可视化训练集前1万个样本。观察PCA和LDA降维后可视化的差异# PCA降维# from sklearn.decomposition import PCA# pca = PCA(n_components=3)# vis_feature = pca.fit_transform(train_image[:10000].reshape(train_image[:10000].shape[0], -1))# LDA降维from sklearn.discriminant_analysis import LinearDiscriminantAnalysis lda = LinearDiscriminantAnalysis(n_components=3)vis_feature = lda.fit_transform(train_image[:10000].reshape(train_image[:10000].shape[0], -1), \                                train_label[:10000].flatten())    vis_feature = pd.DataFrame(vis_feature, columns=['x1', 'x2', 'x3'])vis_feature['label'] = train_label[:10000].flatten()sns.relplot(data=vis_feature, x='x1', y='x2', hue='label',                 palette=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\                          '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])#%% 数据集population = train_image.shape[0]idx = np.random.permutation(population)# 划分训练集、验证集、测试集，各有 55000, 5000, 10000 个样本trainset = MNISTDataset(train_image[idx[:55000]], train_label[idx[:55000]], tag='train')valset = MNISTDataset(train_image[idx[55000:]], train_label[idx[55000:]], tag='validation', \                             feature_mean=trainset.mean, feature_std=trainset.std)    testset = MNISTDataset(test_image, test_label, tag='test', \                             feature_mean=trainset.mean, feature_std=trainset.std)#%% 模型from model import MLPhidden_size = 32model = MLP(28*28, hidden_size, 10)#%% 训练# TODO: 实现交叉熵代价函数from train import trainer# 训练器配置参数config = {'epoches': 6, 'batchsize':100, 'lr':0.02, 'momentum':0.95}trainloss, macro_F1, weighted_F1, snapshots = trainer(trainset, valset, model, config, save_snapshot=True)plt.figure()plt.plot(trainloss)plt.xlabel('iteration')plt.ylabel('loss')plt.title('training process')#%% 测试from utils import confusion_matrix, classification_metric, show_confusion_matrixsns.set_style("white")best_parameters = snapshots[np.argmax(weighted_F1)]model.load_state_dict(best_parameters)test_feature, test_label = testset[:]with th.no_grad():    test_output = model(test_feature)    test_pred = th.argmax(test_output, dim=1, keepdim=True)cmatrix = confusion_matrix(test_pred, test_label, 10)precision, recall, F1, weights = classification_metric(test_pred, test_label, 10)show_confusion_matrix(cmatrix.numpy())print(f'average precision: {precision.mean().item():.3f}, average recall: {recall.mean().item():.3f},\      macro F1: {F1.mean().item():.3f}, weighted average F1: {sum(F1*weights).item():.3f}')            #%% 隐含层权重可视化sns.set_style("dark")W1 = model.hidden_layer.weight.detach().numpy()plt.figure()for i in range(hidden_size):    plt.subplot(6, 6, i+1)    plt.xticks([])    plt.yticks([])    plt.imshow(W1[i,:].reshape((28,28)), cmap='gray')plt.show()#%% 观察错误分类的样本及模型输出sns.set_style("darkgrid")diff = test_pred != test_labelerr_idx = np.where(diff)[0]# 观察前 N 个错误分类N = 10origin_test_feature = (test_feature * trainset.std) + trainset.meantest_output_prob = th.softmax(test_output, dim=1).detach().numpy()for i in range(N):    im = origin_test_feature[err_idx[i], :].reshape((28, 28))    plt.figure()    plt.subplot(2, 1, 1)    plt.xticks([])    plt.yticks([])    plt.title('Label: {0}, Prediction: {1}'.format(test_label[err_idx[i],0], test_pred[err_idx[i], 0]))    plt.imshow(im, cmap='gray', interpolation='None')        plt.subplot(2, 1, 2)    plt.xticks([0,1,2,3,4,5,6,7,8,9])    plt.ylabel('Output Probability')    plt.bar(np.arange(10), test_output_prob[err_idx[i], :])        plt.show()