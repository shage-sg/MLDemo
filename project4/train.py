# -*- coding: utf-8 -*-import torch as thimport copyfrom torch import optimfrom torch.utils.data import DataLoaderfrom torch.nn import CrossEntropyLossfrom utils import classification_metricfrom torch.nn import LogSoftmaxfrom torch.nn import NLLLossdef cross_entropy(prediction, label):    return NLLLoss()(LogSoftmax(dim=1)(prediction), label.flatten())def trainer(trainset, valset, model, config, save_snapshot=False):    trainloss = []    macro_F1 = []    weighted_F1 = []    snapshots = []  # 保存模型快照    train_loader = DataLoader(trainset, batch_size=config['batchsize'], shuffle=True)    val_feature, val_label = valset[:]    optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'])    lossfunc = CrossEntropyLoss()    for epoch in range(config['epoches']):        iteration = 0        for feature, label in train_loader:            # forward step            prediction = model(feature)            # 解除注释，利用 PyTorch 的 CrossEntropyLoss 检验实现的正确性            # loss = lossfunc(prediction, label.flatten())            loss = cross_entropy(prediction, label)            trainloss.append(loss.item())            iteration += 1            if iteration % 10 == 0:                with th.no_grad():                    val_pred = th.argmax(model(val_feature), dim=1, keepdim=True)                precision, recall, F1, weights = classification_metric(val_pred, val_label, 10)                macro_F1.append(F1.mean().item())  # 算术平均                weighted_F1.append(sum(F1 * weights).item())  # 加权和                print(f'epoch: {epoch} iteration: {iteration:3d}, macro F1: {macro_F1[-1]:.3f},\                      weighted F1: {weighted_F1[-1]:.3f}')            # backward step            optimizer.zero_grad()            loss.backward()            # update parameters            optimizer.step()            # save snapshot            if save_snapshot:                snapshots.append(copy.deepcopy(model.state_dict()))        # 学习率每一轮递减        for g in optimizer.param_groups:            g['lr'] *= 0.7    return trainloss, macro_F1, weighted_F1, snapshots