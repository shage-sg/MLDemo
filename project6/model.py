# -*- coding: utf-8 -*-import matplotlib.pyplot as pltimport numpy as npimport pandas as pd# from scipy.stats import multivariate_normalfrom utils import euclidean_distance_matrix, cosine_distance_matrix, calinski_harabasz_scoreclass KMeans(object):    def __init__(self, K, tol=1e-4, max_iter=200):        self.K = K        self.tol = tol        self.max_iter = max_iter        self.snapshot = []    def fit(self, data):        n_sample = data.shape[0]        # randomly choose centroids        idx = np.random.choice(n_sample, self.K)        self.centroids = data[idx]        self.label = np.empty(n_sample, dtype=np.int16)        for i in range(self.max_iter):            # 指派每个样本所属的簇。（利用 predict 方法）            self.label = np.array([self.predict(data=i) for i in data])            old_centroids = self.centroids.copy()            # save snapshots            self.snapshot.append((self.centroids.copy(), self.label.copy()))            # update step            # 更新每个簇中心的坐标            self.centroids = np.array([np.mean(data[self.label == center], axis=0) for center in range(self.K)])            # check convergence            adjustment = np.mean((old_centroids - self.centroids) ** 2)            CH_score = calinski_harabasz_score(data, self.label)            print(f'Iteration: {i + 1}, CH score: {CH_score:.3f}')            if adjustment < self.tol:                return    def predict(self, data):        distances = np.linalg.norm(self.centroids - data, ord=2, axis=1)        return np.argmin(distances)class DPC(object):    def __init__(self, data, dc_threshold=6, dist_metric='euclidean'):        # dc_threshold 是重要的超参数        self.data = data        self.dc_threshold = dc_threshold        self.dist_metric = dist_metric        self.D = None        self.dc = None        self.rho = None        self.delta = None        self.gamma = None        self.label = None        self.centroids = None    def fit(self):        if self.dist_metric == 'euclidean':            self.D = euclidean_distance_matrix(self.data)        elif self.dist_metric == 'cosine':            self.D = cosine_distance_matrix(self.data)        # 根据经验，利用 dc_threshold 决定 dc        triu = np.triu(self.D, 1)        self.dc = np.percentile([i for i in triu.flatten().tolist() if i != 0], self.dc_threshold)        # compute rho        self.rho = np.count_nonzero((self.D - self.dc) < 0, axis=1) - 1        # compute delta        self.delta = np.zeros((self.data.shape[0]), dtype=np.float32)        for i in range(self.data.shape[0]):            idx = self.rho > self.rho[i]            if np.any(idx):                self.delta[i] = self.D[i, idx].min()            else:                self.delta[i] = self.D[i].max()        # compute gamma        self.gamma = self.rho * self.delta    def assign_cluster(self, K):        # 簇的数量由人工观察后指定        cidx = np.argsort(-self.gamma)        # extract centroids        self.centroids = self.data[cidx[:K]]        # 广度优先算法给数据点指派标签，-1代表样本空间的离群点        self.label = -np.ones(self.data.shape[0], dtype=np.int32)        Q = []        for i in range(K):            self.label[cidx[i]] = i            Q.append(cidx[i])        while len(Q) > 0:            ref = int(Q.pop(0))            cand = np.argwhere(np.logical_and(self.D[ref] < self.dc, self.label == -1))            ridx = np.argsort(self.D[ref][cand].flatten())            for neighbor in cand[ridx].flatten():                self.label[neighbor] = self.label[ref]                Q.append(neighbor)        # show metric score        CH_score = calinski_harabasz_score(self.data, self.label, self.centroids)        print(f'CH score: {CH_score:.3f}')    def visualize_cluster(self, size=10):        # 设定簇颜色        cdict = {-1: np.array([0, 0, 0])}  # 离群点是黑色        for i in range(self.centroids.shape[0]):            cdict[i] = np.random.rand(3)        # 设置样本点颜色        colors = [cdict[l] for l in self.label]        if self.data.shape[1] == 2:            X = self.data        elif self.data.shape[1] > 2:            # 降维            from sklearn.decomposition import PCA            pca = PCA(n_components=2)            X = pca.fit_transform(self.data)        else:            return        plt.figure()        plt.scatter(X[:, 0], X[:, 1], c=colors, s=size)        plt.title('Clusters')        plt.show()        # show density        plt.figure()        plt.scatter(X[:, 0], X[:, 1], c=self.rho, s=size, cmap='Greens')        plt.title('Density')        plt.show()    def show_decision_graph(self):        plt.figure()        plt.scatter(self.rho, self.delta)        plt.title('Decision graph')        plt.show()        plt.figure()        plt.plot(np.sort(self.gamma)[::-1], 'k.')        plt.title('Gamma')        plt.show()if __name__ == '__main__':    data = pd.read_csv('shape_sets/flame.txt', sep='\t', header=None)    X = data.loc[:, :1].to_numpy()    X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))    # model = DPC(X, dc_threshold=6, dist_metric='euclidean')    model = KMeans(K=2)    model.fit(X)    model.show_decision_graph()    model.assign_cluster(2)    model.visualize_cluster()    calinski_harabasz_score(X, model.label, model.centroids)