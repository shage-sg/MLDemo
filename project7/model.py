# -*- coding: utf-8 -*-import numpy as npimport osimport re# 利用国内源安装分词引擎：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple thulacimport thulacclass NaiveBayes(object):    def __init__(self, dataset, wdict, stopwords, corpusroot):        self.dataset = dataset # 数据集        self.wdict = wdict  # 词典        self.stopwords = stopwords  # 停用词        self.corpusroot = corpusroot # 语料库根目录                # 生成词袋向量表示 bow = bag of words    def generate_bow(self, dataset):        thu = thulac.thulac(seg_only=True) # 中文分词引擎        bow_code = np.zeros((len(dataset), len(self.wdict)), dtype=np.int32)                for rid, row in dataset.iterrows():            file = row['fpath']                    with open(os.path.join(self.corpusroot, file), 'r', encoding='gb18030', errors='ignore') as fh:                txt = fh.read()                        # 去除非中文字符            txt = re.sub(r'[^\u4e00-\u9fa5]+', '', txt)                    # 分词            text = thu.cut(txt, text=True)            words = set(text.split())                        # 去除停用词            words.difference_update(self.stopwords)                        # 编码            # TODO: 利用词典 wdict 建立 words 的词袋表示 bow_code。（略过不在词典里的词）                                        if rid % 1000 == 0:                print(f'Processing {rid}/{len(dataset)}')        return bow_code            def build(self):        bow_code = self.generate_bow(self.dataset)        label = self.dataset.loc[:,'label'].to_numpy()                # 计算 P(spam) 和 P(ham)        self.prob_spam = np.sum(label==1) / len(label)        self.prob_ham = 1 - self.prob_spam                # 计算 P(x|spam) 和 P(x|ham)，并进行拉普拉斯平滑        self.cond_spam = (bow_code[label==1, :].sum(axis=0) + 1) / (np.sum(label==1) + 2)        self.cond_ham = (bow_code[label==0, :].sum(axis=0) + 1) / (np.sum(label==0) + 2)                    